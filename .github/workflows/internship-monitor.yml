name: Internship Monitor

on:
  schedule:
    # Runs every 3 minutes (UTC)
    - cron: "*/5 * * * *"
  workflow_dispatch:  # allows manual trigger
  repository_dispatch:
    types: [run_scraper]

jobs:
  run-monitor:
    runs-on: ubuntu-latest
    permissions: 
      contents: write

    steps:
    - name: Check out repo
      uses: actions/checkout@v4
      with:
        # Fetch full history to allow commits
        fetch-depth: 0
        persist-credentials: false

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.x"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4

    - name: Run scraper script
      env:
        DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
      run: |
        python scraper.py

    - name: Commit updated seen_links.json
      if: always()
      run: |
        git config --local user.email "actions@github.com"
        git config --local user.name "GitHub Actions"
        git add seen_links.json
        # Only commit if there are changes
        git diff --cached --quiet || git commit -m "Update seen links"
        # Push using GITHUB_TOKEN for authentication
        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git

    - name: Force keep-alive commit
      run: |
        echo "Last run: $(date -u)" > .keepalive
        git config --local user.email "actions@github.com"
        git config --local user.name "GitHub Actions"
        git add .keepalive seen_links.json
        git commit -m "Keep-alive commit" || true
        git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git
        
